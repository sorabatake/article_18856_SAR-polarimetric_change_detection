{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests\n",
    "import glob\n",
    "import numpy as np\n",
    "import re\n",
    "import struct\n",
    "# 既存のTellus環境にgeocoderは入ってないので、!pip install geocoderでインストールしてください\n",
    "import geocoder\n",
    "\n",
    "# Fields\n",
    "BASE_API_URL = \"https://file.tellusxdp.com/api/v1/origin/search/\" # https://www.tellusxdp.com/docs/api-reference/palsar2-files-v1.html#/\n",
    "ACCESS_TOKEN = \"*******************\" # Tellus登録時のアクセストークンを入れてください\n",
    "HEADERS = {\"Authorization\": \"Bearer \" + ACCESS_TOKEN}\n",
    "TARGET_PLACE=\"tokyo\" # ここに取得した地名を入れてください\n",
    "SAVE_DIRECTORY=\"./SAR_{}/\".format(TARGET_PLACE)\n",
    "\n",
    "# Functions\n",
    "def rect_vs_point(ax, ay, aw, ah, bx, by):\n",
    "    return 1 if bx > ax and bx < aw and by > ay and by < ah else 0\n",
    "def get_scene_list_slc(_get_params={}):\n",
    "    query = \"palsar2-l11\"\n",
    "    r = requests.get(BASE_API_URL + query, _get_params, headers=HEADERS)\n",
    "    if not r.status_code == requests.codes.ok:\n",
    "        r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def get_slc_scenes(_target_json, _get_params={}):\n",
    "    # get file list\n",
    "    r = requests.get(_target_json[\"publish_link\"], _get_params, headers=HEADERS)\n",
    "    if not r.status_code == requests.codes.ok:\n",
    "        r.raise_for_status()\n",
    "    file_list = r.json()['files']\n",
    "    dataset_id = _target_json['dataset_id'] # folder name\n",
    "    # make dir\n",
    "    if os.path.exists(SAVE_DIRECTORY + dataset_id) == False:\n",
    "        os.makedirs(SAVE_DIRECTORY + dataset_id)\n",
    "    # downloading\n",
    "    print(\"[Start downloading]\", dataset_id)\n",
    "    for _tmp in file_list:\n",
    "        r = requests.get(_tmp['url'], headers=HEADERS, stream=True)\n",
    "        if not r.status_code == requests.codes.ok:\n",
    "            r.raise_for_status()\n",
    "        with open(os.path.join(SAVE_DIRECTORY, dataset_id, _tmp['file_name']), \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        print(\"  - [Done]\", _tmp['file_name'])\n",
    "    print(\"finished\") \n",
    "    return\n",
    "\n",
    "# Entry point\n",
    "def main():\n",
    "    # extract slc list around the address\n",
    "    gc = geocoder.osm(TARGET_PLACE, timeout=5.0) # get latlon\n",
    "    slc_list_json = get_scene_list_slc({'after':'2019-1-01T00:00:00', 'polarisations':'HH+HV+VV+VH', 'page_size':'1000'})  # get single look complex list\n",
    "    target_places_json = [_ for _ in slc_list_json['items'] if rect_vs_point(_['bbox'][1], _['bbox'][0], _['bbox'][3], _['bbox'][2], gc.latlng[0], gc.latlng[1])] # lot_min, lat_min, lot_max...\n",
    "    target_ids = [_['dataset_id'] for _ in target_places_json]\n",
    "    print(\"[Matched SLCs]\", target_ids)\n",
    "    for target_id in target_ids:\n",
    "        target_json = [_ for _ in slc_list_json['items'] if _['dataset_id'] == target_id][0]\n",
    "        # download the target file\n",
    "        get_slc_scenes(target_json)\n",
    "        #break # if remove, download all data\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "       main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上記の結果を参考にして、今回は以下の二つのSARデータを取り出すことにしました。\n",
    "target_id1 = 'SAR_tokyo/ALOS2267472890-190507'\n",
    "target_id2 = 'SAR_tokyo/ALOS2250912890-190115'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Get_image():\n",
    "    def __init__(self, dataset_id=''):\n",
    "        self.dataset_id = dataset_id\n",
    "    \n",
    "    def SAR_data(self):\n",
    "        names = ['HH','HV','VV']\n",
    "        lists = glob.glob(self.dataset_id + '/*')\n",
    "        CEOS_lists = sorted({k for k in lists if re.search('IMG-(HH|HV|VV)', str(k))})\n",
    "        i=0\n",
    "        for filename in CEOS_lists:\n",
    "            fp = open(os.path.join('/home','jovyan','work',filename),mode='rb')\n",
    "            col = 2000\n",
    "            row = 2000\n",
    "            cr = np.array([3000,3000+col,8000,8000+row],dtype=\"i4\");\n",
    "            \n",
    "            fp.seek(236)\n",
    "            nline = int(fp.read(8))\n",
    "            fp.seek(248)\n",
    "            ncell = int(fp.read(8))\n",
    "            nrec = 544 + ncell*8\n",
    "\n",
    "            nline = cr[3]-cr[2]\n",
    "            fp.seek(720)\n",
    "            fp.read(int((nrec/4)*(cr[2])*4))\n",
    "            data = struct.unpack(\">%s\"%(int((nrec*nline)/4))+\"f\",fp.read(int(nrec*nline)))\n",
    "            data = np.array(data).reshape(-1,int(nrec/4))\n",
    "            data = data[:,int(544/4):int(nrec/4)]\n",
    "            slc = data[:,::2] + 1j*data[:,1::2]\n",
    "            slc = slc[:,cr[0]:cr[1]]\n",
    "            print(np.shape(slc))\n",
    "            np.save(os.path.join('/home','jovyan','work',self.dataset_id,names[i]),slc)\n",
    "            i+=1\n",
    "\n",
    "\n",
    "SAR_get = Get_image(target_id1)\n",
    "SAR_get.SAR_data()\n",
    "print('GET SAR SLC DATA1 AS NUMPY!!')\n",
    "\n",
    "SAR_get = Get_image(target_id2)\n",
    "SAR_get.SAR_data()\n",
    "print('GET SAR SLC DATA2 AS NUMPY!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果の可視化(確認用)\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import inf\n",
    "import numpy as np\n",
    "\n",
    "Shh = np.load('/home/jovyan/work/'+ target_id1 + '/HH.npy')\n",
    "Shv = np.load('/home/jovyan/work/'+ target_id1 + '/HV.npy')\n",
    "Svv = np.load('/home/jovyan/work/'+ target_id1 + '/VV.npy')\n",
    "\n",
    "sigma = 10*np.log10(Shh) -83.0 -32.0\n",
    "sigma = np.array(255*(sigma-np.amin(sigma))/(np.amax(sigma)-np.amin(sigma)),dtype=\"uint8\")\n",
    "HH = sigma\n",
    "\n",
    "sigma = 10*np.log10(Shv) -83.0 -32.0\n",
    "sigma = np.array(255*(sigma-np.amin(sigma))/(np.amax(sigma)-np.amin(sigma)),dtype=\"uint8\")\n",
    "HV = sigma\n",
    "\n",
    "sigma = 10*np.log10(Svv) -83.0 -32.0\n",
    "sigma = np.array(255*(sigma-np.amin(sigma))/(np.amax(sigma)-np.amin(sigma)),dtype=\"uint8\")\n",
    "VV = sigma\n",
    "\n",
    "\n",
    "img=np.zeros((2000, 2000, 3), np.uint8)\n",
    "img[:,:,0]=HH\n",
    "img[:,:,1]=HV\n",
    "img[:,:,2]=VV\n",
    "\n",
    "# ルックアップテーブルの生成(輝度の調整)\n",
    "min_table = 70\n",
    "max_table = 255\n",
    "diff_table = max_table - min_table\n",
    "look_up_table = np.arange(256, dtype = 'uint8')\n",
    "for i in range(0, min_table):\n",
    "    look_up_table[i] = 0\n",
    "for i in range(min_table, max_table):\n",
    "    look_up_table[i] = 255 * (i - min_table) / diff_table\n",
    "for i in range(max_table, 255):\n",
    "    look_up_table[i] = 255\n",
    "\n",
    "img = cv2.LUT(img, look_up_table)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(cv2.flip(img,1))\n",
    "plt.imsave('img1.png', cv2.flip(img,1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
